{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of randomForrest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paul080791/lime/blob/paul080791-patch-1/Copy_of_randomForrest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISvcEiTZ1QnP",
        "outputId": "dccfef2c-19df-44af-9de1-99ee1fd6799d"
      },
      "source": [
        "#load data \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests \n",
        "import tarfile\n",
        "import os \n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "url = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz\"\n",
        "filename = \"processed_acl\"\n",
        "extension = \".tar.gz\"\n",
        "\n",
        "response = requests.get(url, allow_redirects=True)\n",
        "\n",
        "print(\"http response:\",response.status_code,response.reason)\n",
        "\n",
        "with open(filename+extension,\"wb\") as file: \n",
        "    file.write(response.content)\n",
        "    \n",
        "tar = tarfile.open(filename+extension, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "# # merge files using shell commands \n",
        "print(os.popen(\"cat \" + filename+\"/books/negative.review > mixed.txt\").read())\n",
        "print(os.popen(\"cat \" + filename+\"/books/positive.review >> mixed.txt\").read())\n",
        "# print(os.popen(\"cat \" + filename+\"/dvd/all.review >> mixed.txt\").read())\n",
        "# print(os.popen(\"cat \" + filename+\"/dvd/positive.review >> mixed.txt\").read())\n",
        "\n",
        "\n",
        "\n",
        "# parse data into array for word 2 vect \n",
        "dataArray = []\n",
        "labelArray = []\n",
        "\n",
        "with open(\"mixed.txt\", \"r\") as datafile: \n",
        "    lines = datafile.readlines()\n",
        "    for e in lines:\n",
        "        lineList= []\n",
        "        splitLine = e.split()\n",
        "        label = splitLine.pop(-1)\n",
        "        for i in splitLine:\n",
        "            dictSplit = i.split(':')\n",
        "            lineList.append(dictSplit[0])\n",
        "        dataArray.insert(-1, lineList)\n",
        "        labelArray.insert(-1, (label.split(':')[1]))\n",
        "\n",
        "#download word2vec model          \n",
        "word2vec = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "\n",
        "# create mean embedding \n",
        "embed = np.array([ np.mean([word2vec[w] for w in words if w in word2vec], axis=0) for words in dataArray])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embed, labelArray, test_size=0.2, random_state=42)\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=700, random_state=42)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http response: 200 OK\n",
            "\n",
            "\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
            "[[164  35]\n",
            " [ 37 164]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.82      0.82       199\n",
            "    positive       0.82      0.82      0.82       201\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5WWhGNC1QnS",
        "outputId": "fb1f3ec0-0cf8-4001-8ea1-5a066ccd9b30"
      },
      "source": [
        "#  dl tubspam data set \n",
        "# load into numpy array \n",
        "# create embedings on full text \n",
        "# use classifier to classify the dataset \n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\"\n",
        "!wget  -c \"https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\"#download it\n",
        "\n",
        "\n",
        "!unzip -o YouTube-Spam-Collection-v1.zip\n",
        "df1 = pd.read_csv(\"/content/Youtube01-Psy.csv\")\n",
        "df2 = pd.read_csv(\"/content/Youtube02-KatyPerry.csv\")\n",
        "df3 = pd.read_csv(\"/content/Youtube03-LMFAO.csv\")\n",
        "df4 = pd.read_csv(\"/content/Youtube04-Eminem.csv\")\n",
        "df5 = pd.read_csv(\"/content/Youtube05-Shakira.csv\")\n",
        "\n",
        "df = pd.concat([df1, df2,df3,df4,df5], axis=0, join='inner')\n",
        "dataArray = []\n",
        "labelArray = []\n",
        "\n",
        "dataArray= df['CONTENT'].values\n",
        "labelArray= df['CLASS'].values\n",
        "\n",
        "\n",
        "#download word2vec model          \n",
        "word2vec = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "\n",
        "# create mean embedding \n",
        "embed = np.array([ np.mean([word2vec[w] for w in words if w in word2vec], axis=0) for words in dataArray])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embed, labelArray, test_size=0.2, random_state=42)\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=700, random_state=42)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-24 19:46:13--  https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163567 (160K) [application/x-httpd-php]\n",
            "Saving to: ‘YouTube-Spam-Collection-v1.zip’\n",
            "\n",
            "YouTube-Spam-Collec 100%[===================>] 159.73K   824KB/s    in 0.2s    \n",
            "\n",
            "2021-04-24 19:46:13 (824 KB/s) - ‘YouTube-Spam-Collection-v1.zip’ saved [163567/163567]\n",
            "\n",
            "--2021-04-24 19:46:13--  http://it/\n",
            "Resolving it (it)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘it’\n",
            "FINISHED --2021-04-24 19:46:13--\n",
            "Total wall clock time: 0.6s\n",
            "Downloaded: 1 files, 160K in 0.2s (824 KB/s)\n",
            "Archive:  YouTube-Spam-Collection-v1.zip\n",
            "  inflating: Youtube01-Psy.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._Youtube01-Psy.csv  \n",
            "  inflating: Youtube02-KatyPerry.csv  \n",
            "  inflating: __MACOSX/._Youtube02-KatyPerry.csv  \n",
            "  inflating: Youtube03-LMFAO.csv     \n",
            "  inflating: __MACOSX/._Youtube03-LMFAO.csv  \n",
            "  inflating: Youtube04-Eminem.csv    \n",
            "  inflating: __MACOSX/._Youtube04-Eminem.csv  \n",
            "  inflating: Youtube05-Shakira.csv   \n",
            "  inflating: __MACOSX/._Youtube05-Shakira.csv  \n",
            "[[160  16]\n",
            " [ 40 176]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       176\n",
            "           1       0.92      0.81      0.86       216\n",
            "\n",
            "    accuracy                           0.86       392\n",
            "   macro avg       0.86      0.86      0.86       392\n",
            "weighted avg       0.86      0.86      0.86       392\n",
            "\n",
            "0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bqvZSIanIi"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwyjhmKE3J9u"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}